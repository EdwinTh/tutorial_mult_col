---
title: "Lab correlated features"
output: html_notebook
---

So we saw quite different estimates when resampling x1 and x2 only once. To get a feel for the impact of correlation between features we are going to do some resampling. 


1.1 Install the `MASS` and `tidyverse` libraries if you haven't done so and load them.

```{r, message=FALSE}
# install.packages('MASS')
# install.packages('tidyverse')
library(MASS)
library(tidyverse)
```

`MASS` contains the `mvrnorm` function that allows sampling from a multivariate normal distribution. x1 and x2 are multivariately distributed with the vector mu as theirs respective means and the covariance matrix Sigma with the variances on the diagonal and the covariance on the offdiagonal.

1.2 Play around with the `mvrnorm` function in the `MASS` package.

```{r}
?mvrnorm
mvrnorm(100, c(10, 20), matrix(c(5, 3, 3, 10), nrow = 2))  %>% cor
mvrnorm(100, c(10, 20), matrix(c(5, 6, 6, 10), nrow = 2))  %>% cor
```

Did you happen to try sampling from a distribution at which the covariances are large compared to the variances?

1.3 If not do so and explain what is happening?

```{r}
mvrnorm(100, c(0, 0), matrix(c(2, 3, 3, 2), nrow = 2))
```

We are trying to sample from a distribution with a correlation bigger than 1. In the case of just two variables this is pretty obvious, but with multiple variables Sigma is also singular with lower correlations. This has a close relation with estimating regression coefficients. Even though the individual correlations are moderately high there are perfect combinations within the feature matrix. Besides a correlation analysis it also a good idea to do a PCA before fitting any model to observational data.

1.4 Build a function that 

a) samples 1000 cases from a standard multivariate normal with two featuer, with the correlation as parameter.
   (hence when x1 and x2 are standardnormally distributed their covariance is also the correlation)
b) creates y as in the slides.
c) regresses y on x1 and x2 and return the three coeffictients.

```{r}
get_estimates <- function(rho = 0) {
  X <- mvrnorm(1000, rep(0, 2), matrix(c(1, rho, rho, 1), nrow = 2)) %>% 
    as_data_frame %>% mutate(y = V1 + V2 + rnorm(1000))
  lm(y ~ ., data = X)$coefficients
}
```

1.5 Now apply this function on a grid between 0 and 1, with step size .1. Do a 100 repetitions for each of the correlation values. Collect the results in a 1100 x 4 data frame, with a column indicating the rho value added.

Before doing the analysis, what do you expect from rho = 1? Why might it be more insightful to take .99 instead?

```{r}
rho_grid <- seq(0, .9, .1) %>% c(.99)
results  <- vector('list', length(rho_grid)) 
for(i in 1:length(rho_grid)) {
  results[[i]] <- replicate(100, get_estimates(rho_grid[i])) %>% t %>% 
    as_data_frame %>% 
    mutate(rho_value = rho_grid[i])
}
result_df <- do.call('rbind', results)
```

1.6 For further analysis it is helpful to place this in long format with three columns, the rho value, the parameter name and the estimate.

```{r}
result_long <- gather(result_df, key = parameter, value = estimate, -rho_value)
```

1.7 Now compare the different row values by calculating statistics. What do you expect in terms of bias and variance of the estimates? If you prefer analyze the statistics grahpically.

```{r}
stats <- result_long %>% group_by(rho_value, parameter) %>% 
  summarise(ave = mean(estimate), 
            std = sd(estimate),
            min = min(estimate),
            max = max(estimate)) %>% 
  gather(key = stat, value = val, -rho_value, -parameter)

ggplot(stats, aes(rho_value, val)) + 
  geom_point(aes(col = stat)) +
  geom_line(aes(col = stat)) + 
  facet_wrap(~parameter)
  
```



